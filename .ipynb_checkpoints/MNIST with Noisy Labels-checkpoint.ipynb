{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BETA_RECON = 0.0005\n",
    "BETA_SOFT = 0.95\n",
    "BETA_HARD = 0.8\n",
    "\n",
    "num_features = 784\n",
    "num_classes = 10\n",
    "hidden_layer_sizes = [500, 300, num_classes]\n",
    "\n",
    "num_iter = 10000\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_features], name=\"inputs\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(X, input_size, output_size, activation_fn=tf.nn.relu, activation_name='activation', linear_name='linear'):\n",
    "    weights = tf.get_variable(\"weights\", [input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [output_size], initializer=tf.constant_initializer(0.0))\n",
    "    if activation_fn:\n",
    "        return activation_fn(tf.add(tf.matmul(X, weights), biases, name=linear_name), name=activation_name)\n",
    "    return tf.add(tf.matmul(X, weights), biases, name=linear_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"fc1\"):\n",
    "#     fc1 = dense(X, num_features, 500, activation_fn=tf.nn.relu)\n",
    "# with tf.variable_scope(\"fc2\"):\n",
    "#     fc2 = dense(fc1, 500, num_classes, activation_fn=None)\n",
    "#     fc2_softmax = tf.nn.softmax(fc2, name='fc2_softmax')\n",
    "    \n",
    "prev_layer, prev_hidden_layer_size = X, num_features\n",
    "for i, hidden_layer_size in enumerate(hidden_layer_sizes, 1):\n",
    "    with tf.variable_scope(\"fc{layer_index}\".format(layer_index=i)):\n",
    "        if i != len(hidden_layer_sizes):\n",
    "            h = dense(prev_layer, prev_hidden_layer_size, hidden_layer_size)\n",
    "            prev_layer, prev_hidden_layer_size = h, hidden_layer_size\n",
    "        else:\n",
    "            logits = dense(prev_layer, prev_hidden_layer_size, num_classes, activation_fn=None, linear_name='true_logits')\n",
    "            probs = tf.nn.softmax(logits, name='true_labels')\n",
    "            \n",
    "with tf.variable_scope(\"recon\"):\n",
    "    recon = dense(probs, num_classes, num_features, activation_fn=None, linear_name='reconstructed_x')\n",
    "with tf.variable_scope(\"noisy_mapping\"):\n",
    "    noisy_probs = dense(probs, num_classes, num_classes, activation_fn=tf.nn.softmax, linear_name='noisy_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")\n",
    "\n",
    "with tf.name_scope(\"baseline\"):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name=\"cross_entropy_loss\")\n",
    "    baseline_train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)    \n",
    "    \n",
    "with tf.name_scope(\"recon\"):\n",
    "    recon_loss = BETA_RECON * tf.nn.l2_loss(X - recon, name='reconstruction_loss')\n",
    "    noisy_cross_entropy_loss = - tf.reduce_sum(tf.multiply(tf.log(noisy_probs + 1e-10), Y), name='noisy_cross_entropy_loss')\n",
    "    l_recon = tf.add(noisy_cross_entropy_loss, recon_loss, name=\"l_recon\")\n",
    "    recon_train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(l_recon)\n",
    "    \n",
    "with tf.name_scope(\"soft_bootstrap\"):\n",
    "    l_soft = - tf.reduce_sum(tf.multiply(BETA_SOFT * Y + (1 - BETA_SOFT) * probs, tf.log(probs + 1e-10)), name='l_soft')\n",
    "    soft_train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(l_soft)\n",
    "\n",
    "with tf.name_scope(\"hard_bootstrap\"):\n",
    "    z = tf.one_hot(tf.argmax(probs, 1, name='fc2_softmax_argmax'), num_classes, name='z')\n",
    "    l_hard = - tf.reduce_sum(tf.multiply(BETA_HARD * Y + (1 - BETA_HARD) * z, tf.log(probs + 1e-10)), name='l_hard')\n",
    "    hard_train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(l_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(probs, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", cross_entropy)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    tf.summary.scalar(\"loss_recon\", l_recon)\n",
    "    tf.summary.scalar(\"reconstruction_loss\", recon_loss)\n",
    "    tf.summary.scalar(\"noisy_cross_entropy_loss\", noisy_cross_entropy_loss)\n",
    "    tf.summary.scalar(\"loss_soft\", l_soft)\n",
    "    tf.summary.scalar(\"loss_hard\", l_hard)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Running Baseline\")\n",
    "    baseline_writer = tf.summary.FileWriter('./graph/baseline', sess.graph)\n",
    "    for i in range(num_iter):\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 1000 == 0:\n",
    "            train_acc = accuracy.eval(feed_dict={X: X_batch, Y: Y_batch})\n",
    "            print(\"Step {0}, Train Acc {1}\".format(i, train_acc))\n",
    "        _, summary = sess.run([baseline_train_step, summary_op], feed_dict={X:X_batch, Y:Y_batch})\n",
    "        baseline_writer.add_summary(summary, global_step=i)\n",
    "    test_acc = accuracy.eval(feed_dict={X: mnist.test.images, Y:mnist.test.labels})\n",
    "    print(\"Test Acc {1}\".format(i, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Recon Bootstrap\n",
      "Step 0, Train Acc 0.0399999991059\n",
      "Step 1000, Train Acc 0.10000000149\n",
      "Step 2000, Train Acc 0.0799999982119\n",
      "Step 3000, Train Acc 0.0799999982119\n",
      "Step 4000, Train Acc 0.0599999986589\n",
      "Step 5000, Train Acc 0.0799999982119\n",
      "Step 6000, Train Acc 0.0399999991059\n",
      "Step 7000, Train Acc 0.0599999986589\n",
      "Step 8000, Train Acc 0.0799999982119\n",
      "Step 9000, Train Acc 0.019999999553\n",
      "Test Acc 0.0988000035286\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Running Recon Bootstrap\")\n",
    "    soft_writer = tf.summary.FileWriter('./graph/recon', sess.graph)\n",
    "    for i in range(num_iter):\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 1000 == 0:\n",
    "            train_acc = accuracy.eval(feed_dict={X: X_batch, Y: Y_batch})\n",
    "            print(\"Step {0}, Train Acc {1}\".format(i, train_acc))\n",
    "        _, summary = sess.run([recon_train_step, summary_op], feed_dict={X:X_batch, Y:Y_batch})\n",
    "        soft_writer.add_summary(summary, global_step=i)\n",
    "    test_acc = accuracy.eval(feed_dict={X: mnist.test.images, Y:mnist.test.labels})\n",
    "    print(\"Test Acc {1}\".format(i, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Running Soft Bootstrap\")\n",
    "    soft_writer = tf.summary.FileWriter('./graph/soft', sess.graph)\n",
    "    for i in range(num_iter):\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 1000 == 0:\n",
    "            train_acc = accuracy.eval(feed_dict={X: X_batch, Y: Y_batch})\n",
    "            print(\"Step {0}, Train Acc {1}\".format(i, train_acc))\n",
    "        _, summary = sess.run([soft_train_step, summary_op], feed_dict={X:X_batch, Y:Y_batch})\n",
    "        soft_writer.add_summary(summary, global_step=i)\n",
    "    test_acc = accuracy.eval(feed_dict={X: mnist.test.images, Y:mnist.test.labels})\n",
    "    print(\"Test Acc {1}\".format(i, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Running Hard Bootstrap\")\n",
    "    hard_writer = tf.summary.FileWriter('./graph/hard', sess.graph)\n",
    "    for i in range(num_iter):\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 1000 == 0:\n",
    "            train_acc = accuracy.eval(feed_dict={X: X_batch, Y: Y_batch})\n",
    "            print(\"Step {0}, Train Acc {1}\".format(i, train_acc))\n",
    "        _, summary = sess.run([hard_train_step, summary_op], feed_dict={X:X_batch, Y:Y_batch})\n",
    "        hard_writer.add_summary(summary, global_step=i)\n",
    "    test_acc = accuracy.eval(feed_dict={X: mnist.test.images, Y:mnist.test.labels})\n",
    "    print(\"Test Acc {1}\".format(i, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
